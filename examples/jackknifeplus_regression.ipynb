{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a0d3edd",
   "metadata": {},
   "source": [
    "# Jackknife+, Jackknife-minmax and CV+"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f88056",
   "metadata": {},
   "source": [
    "In this notebook we compare `Jackknife+`, `Jackknife-minmax` and `CV+` from [Barber et al. 2021](https://projecteuclid.org/journals/annals-of-statistics/volume-49/issue-1/Predictive-inference-with-the-jackknife/10.1214/20-AOS1965.full)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c58a291",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c248dd",
   "metadata": {},
   "source": [
    "## Generate regression data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb76f9d7",
   "metadata": {},
   "source": [
    "We generate an arbitrary regression data set with scalar target variables. We split it into train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7081576f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "X, y = make_regression(n_samples=2000, n_features=3, n_targets=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e1679e",
   "metadata": {},
   "source": [
    "We arbitrarily decide to adopt a gradient boosting method for regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2c09b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36d787e",
   "metadata": {},
   "source": [
    "We decide for an arbitrary desired coverage of 95%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a8166ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "error = 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2bb11a",
   "metadata": {},
   "source": [
    "## CV+"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6181e88a",
   "metadata": {},
   "source": [
    "First, we train the model using a K-fold cross validation procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "095648db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split #5 out of 5.\r"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "cross_val_outputs, cross_val_targets, cross_test_outputs = [], [], []\n",
    "n_splits = 5\n",
    "for i, idx in enumerate(KFold(n_splits=n_splits).split(X_train)):\n",
    "    print(f\"Split #{i + 1} out of {n_splits}.\", end='\\r')\n",
    "    model = GradientBoostingRegressor()\n",
    "    model.fit(X_train[idx[0]], y_train[idx[0]])\n",
    "    cross_val_outputs.append(model.predict(X_train[idx[1]]))\n",
    "    cross_val_targets.append(y_train[idx[1]])\n",
    "    cross_test_outputs.append(model.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d35e64",
   "metadata": {},
   "source": [
    "Given the model outputs, we compute conformal intervals obtained using CV+."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "050ef152",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fortuna.conformal.regression import CVPlusConformalRegressor\n",
    "cvplus_interval = CVPlusConformalRegressor().conformal_interval(\n",
    "        cross_val_outputs=cross_val_outputs,\n",
    "        cross_val_targets=cross_val_targets,\n",
    "        cross_test_outputs=cross_test_outputs,\n",
    "        error=error\n",
    ")\n",
    "cvplus_coverage = sum((y_test > cvplus_interval[:, 0]) & (y_test < cvplus_interval[:, 1])) / X_test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2c7fe4",
   "metadata": {},
   "source": [
    "# Jackknife+ and jackknife-minmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c031387",
   "metadata": {},
   "source": [
    "We now train the model with a leave-one-out procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eba51fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split #1000 out of 1000.\r"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import LeaveOneOut\n",
    "import jax.numpy as jnp\n",
    "loo_val_outputs, loo_val_targets, loo_test_outputs = [], [], []\n",
    "for i, idx in enumerate(LeaveOneOut().split(X_train)):\n",
    "    print(f\"Split #{i + 1} out of {X_train.shape[0]}.\", end='\\r')\n",
    "    model = GradientBoostingRegressor()\n",
    "    model.fit(X_train[idx[0]], y_train[idx[0]])\n",
    "    loo_val_outputs.append(model.predict(X_train[idx[1]]))\n",
    "    loo_val_targets.append(y_train[idx[1]])\n",
    "    loo_test_outputs.append(model.predict(X_test))\n",
    "loo_val_outputs = jnp.array(loo_val_outputs)\n",
    "loo_val_targets = jnp.array(loo_val_targets)\n",
    "loo_test_outputs = jnp.array(loo_test_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6216915f",
   "metadata": {},
   "source": [
    "Given the model outputs, we compute conformal intervals obtained using jackknife+ jackknife-minmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c2b7b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fortuna.conformal.regression import JackknifePlusConformalRegressor, JackknifeMinmaxConformalRegressor\n",
    "jkplus_interval = JackknifePlusConformalRegressor().conformal_interval(\n",
    "        loo_val_outputs=loo_val_outputs,\n",
    "        loo_val_targets=loo_val_targets,\n",
    "        loo_test_outputs=loo_test_outputs,\n",
    "        error=error\n",
    ")\n",
    "jkplus_coverage = sum((y_test > jkplus_interval[:, 0]) & (y_test < jkplus_interval[:, 1])) / X_test.shape[0]\n",
    "\n",
    "jkmm_interval = JackknifeMinmaxConformalRegressor().conformal_interval(\n",
    "        loo_val_outputs=loo_val_outputs,\n",
    "        loo_val_targets=loo_val_targets,\n",
    "        loo_test_outputs=loo_test_outputs,\n",
    "        error=error\n",
    ")\n",
    "jkmm_coverage = sum((y_test > jkmm_interval[:, 0]) & (y_test < jkmm_interval[:, 1])) / X_test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0441b1f",
   "metadata": {},
   "source": [
    "## Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6819886d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Desired coverage: 0.95.\n",
      "CV+ empirical coverage: 0.9819999933242798.\n",
      "jackknife+ empirical coverage: 0.968999981880188.\n",
      "jackknife-minmax empirical coverage: 0.9890000224113464.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Desired coverage: {1 - error}.\")\n",
    "print(f\"CV+ empirical coverage: {cvplus_coverage}.\")\n",
    "print(f\"jackknife+ empirical coverage: {jkplus_coverage}.\")\n",
    "print(f\"jackknife-minmax empirical coverage: {jkmm_coverage}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f80754c",
   "metadata": {},
   "source": [
    "All methods perform competitively. Compared to CV+, where we trained the model with a 5-fold cross validation, jackknife+ and jackknife-minmax required significantly higher computational time because of the leave-one-out procedure over the whole training data set. Clearly, one may significantly reduce this cost by performing leave-one-out only on a subset of the training data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fortuna",
   "language": "python",
   "name": "fortuna"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
